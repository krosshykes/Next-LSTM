{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-25T05:41:30.554241Z",
     "iopub.status.busy": "2020-10-25T05:41:30.553498Z",
     "iopub.status.idle": "2020-10-25T05:41:30.796973Z",
     "shell.execute_reply": "2020-10-25T05:41:30.795857Z"
    },
    "papermill": {
     "duration": 0.285414,
     "end_time": "2020-10-25T05:41:30.797113",
     "exception": false,
     "start_time": "2020-10-25T05:41:30.511699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import copy\n",
    "import joblib\n",
    "import copy\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.applications import Xception, InceptionV3, ResNet50, ResNet50V2, VGG16\n",
    "from keras.layers import TimeDistributed, Activation, RepeatVector, Concatenate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'images_path': 'train_data/Flicker8k_Dataset/',\n",
    "    'train_data_path': 'train_data/Flickr_8k.trainImages.txt',\n",
    "    'val_data_path': 'train_data/Flickr_8k.devImages.txt',\n",
    "    'captions_path': 'train_data/Flickr8k.token.txt',\n",
    "    'features_path': 'model_data/features.joblib',\n",
    "    'model_data_path': 'model_data/',\n",
    "    'model_load_path': 'model_data/model_inceptionv3_epoch-20_train_loss-2.4050_val_loss-3.0527.hdf5',\n",
    "    'num_of_epochs': 50,\n",
    "    'max_length': 40,  # This is set manually after training of model and required for test.py\n",
    "    'batch_size': 3,\n",
    "    'test_data_path': 'test_data/',\n",
    "    'model_type': 'xception',\n",
    "    'tokenizer_path': 'model_data/tokenizer.joblib',\n",
    "    'random_seed': 5\n",
    "}\n",
    "\n",
    "lstmConfig = {\n",
    "    'embedding_size': 128\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-25T05:41:30.954846Z",
     "iopub.status.busy": "2020-10-25T05:41:30.954090Z",
     "iopub.status.idle": "2020-10-25T05:41:31.238524Z",
     "shell.execute_reply": "2020-10-25T05:41:31.237623Z"
    },
    "papermill": {
     "duration": 0.324605,
     "end_time": "2020-10-25T05:41:31.238656",
     "exception": false,
     "start_time": "2020-10-25T05:41:30.914051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    return os.listdir(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model for Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-25T05:41:32.738626Z",
     "iopub.status.busy": "2020-10-25T05:41:32.737933Z",
     "iopub.status.idle": "2020-10-25T05:41:44.360465Z",
     "shell.execute_reply": "2020-10-25T05:41:44.359211Z"
    },
    "papermill": {
     "duration": 11.683826,
     "end_time": "2020-10-25T05:41:44.360603",
     "exception": false,
     "start_time": "2020-10-25T05:41:32.676777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CNNModel(model_type):\n",
    "    models = {\n",
    "        'inceptionv3': InceptionV3(include_top=True),\n",
    "        'xception': Xception(include_top=True),\n",
    "        'vgg16': VGG16(include_top=True),\n",
    "        'resnet50': ResNet50(include_top=True),\n",
    "        'resnet50v2': ResNet50V2(include_top=True)\n",
    "    }\n",
    "    size = 224\n",
    "    if model_type == 'xception':\n",
    "        size = 299\n",
    "    base_model = models[model_type]\n",
    "    last = base_model.layers[-2].output\n",
    "    main_model = Model(inputs=base_model.input, outputs=last)\n",
    "    return main_model, size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-25T05:41:44.737106Z",
     "iopub.status.busy": "2020-10-25T05:41:44.736018Z",
     "iopub.status.idle": "2020-10-25T05:43:14.924155Z",
     "shell.execute_reply": "2020-10-25T05:43:14.922901Z"
    },
    "papermill": {
     "duration": 90.259706,
     "end_time": "2020-10-25T05:43:14.924281",
     "exception": false,
     "start_time": "2020-10-25T05:41:44.664575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(path, model_type):\n",
    "    features = {}\n",
    "    model, size = CNNModel(model_type)\n",
    "    images = load_images(path)\n",
    "    for i in images:\n",
    "        path = config['images_path']+i\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (size, size))\n",
    "        img = img.reshape(1, size, size, 3)\n",
    "        pred = model.predict(img).reshape(2048,)\n",
    "        image_id = i.split('.')[0]\n",
    "        features[image_id] = pred\n",
    "    return features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071863,
     "end_time": "2020-10-25T05:43:15.226531",
     "exception": false,
     "start_time": "2020-10-25T05:43:15.154668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions(filename):\n",
    "    file = open(filename, 'r')\n",
    "    doc = file.read()\n",
    "    file.close()\n",
    "    captions = dict()\n",
    "    # Process lines by line\n",
    "    _count = 0\n",
    "    for line in doc.split('\\n'):\n",
    "        # Split line on white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # Take the first token as the image id, the rest as the caption\n",
    "        image_id, image_caption = tokens[0], tokens[1:]\n",
    "        # Extract filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # Convert caption tokens back to caption string\n",
    "        image_caption = ' '.join(image_caption)\n",
    "        # Create the list if needed\n",
    "        if image_id not in captions:\n",
    "            captions[image_id] = list()\n",
    "        # Store caption\n",
    "        captions[image_id].append(image_caption)\n",
    "        _count = _count+1\n",
    "    print('Parsed captions: {}'.format(_count))\n",
    "    return captions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_captions(captions):\n",
    "    # Prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for _, caption_list in captions.items():\n",
    "        for i in range(len(caption_list)):\n",
    "            caption = caption_list[i]\n",
    "            # Tokenize i.e. split on white spaces\n",
    "            caption = caption.split()\n",
    "            # Convert to lowercase\n",
    "            caption = [word.lower() for word in caption]\n",
    "            # Remove punctuation from each token\n",
    "            caption = [w.translate(table) for w in caption]\n",
    "            # Remove hanging 's' and 'a'\n",
    "            caption = [word for word in caption if len(word) > 1]\n",
    "            # Remove tokens with numbers in them\n",
    "            caption = [word for word in caption if word.isalpha()]\n",
    "            # Store as string\n",
    "            caption_list[i] = ' '.join(caption)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_captions(captions, filename):\n",
    "    lines = list()\n",
    "    for key, captions_list in captions.items():\n",
    "        for caption in captions_list:\n",
    "            lines.append(key + ' ' + caption)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData():\n",
    "    print('Using {} model'.format(config['model_type'].title()))\n",
    "    # Extract features from all images\n",
    "    fName = 'features_'+config['model_type']+'.joblib'\n",
    "    pText = ''\n",
    "    if os.path.exists(config['model_data_path']+fName):\n",
    "        pText = 'Image features already generated at '\n",
    "        print(pText + config['model_data_path']+fName)\n",
    "    else:\n",
    "        pText = 'Generating image features using '\n",
    "        print(pText+config['model_type']+' model...')\n",
    "        images_features = extract_features(\n",
    "            config['images_path'], config['model_type'])\n",
    "\n",
    "        joblib.dump(images_features, config['model_data_path']+fName)\n",
    "        pText = 'Completed & Saved features for {} images successfully'\n",
    "        print(pText.format(len(images_features)))\n",
    "    # Load file containing captions and parse them\n",
    "    if os.path.exists(config['model_data_path']+'captions.txt'):\n",
    "        pText = 'Parsed caption file already generated at '\n",
    "        print(pText+config['model_data_path']+'captions.txt')\n",
    "    else:\n",
    "        print('Parsing captions file...')\n",
    "        captions = load_captions(config['captions_path'])\n",
    "        # Clean captions\n",
    "        # Ignore this function because Tokenizer from keras will handle cleaning\n",
    "        # clean_captions(captions)\n",
    "        # Save captions\n",
    "        save_captions(captions, config['model_data_path']+'captions.txt')\n",
    "        print('Parsed & Saved successfully')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "    file = open(filename, 'r')\n",
    "    doc = file.read()\n",
    "    file.close()\n",
    "    ids = list()\n",
    "    # Process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # Skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # Get the image identifier(id)\n",
    "        _id = line.split('.')[0]\n",
    "        ids.append(_id)\n",
    "    return set(ids)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_captions(filename, ids):\n",
    "    file = open(filename, 'r')\n",
    "    doc = file.read()\n",
    "    file.close()\n",
    "    captions = dict()\n",
    "    _count = 0\n",
    "    # Process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # Split line on white space\n",
    "        tokens = line.split()\n",
    "        # Split id from caption\n",
    "        image_id, image_caption = tokens[0], tokens[1:]\n",
    "        # Skip images not in the ids set\n",
    "        if image_id in ids:\n",
    "            # Create list\n",
    "            if image_id not in captions:\n",
    "                captions[image_id] = list()\n",
    "            # Wrap caption in start & end tokens\n",
    "            caption = 'startseq ' + ' '.join(image_caption) + ' endseq'\n",
    "            # Store\n",
    "            captions[image_id].append(caption)\n",
    "            _count = _count+1\n",
    "    return captions, _count\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_features(filename, ids):\n",
    "    # load all features\n",
    "    all_features = joblib.load(filename)\n",
    "    # filter features\n",
    "    features = {_id: all_features[_id] for _id in ids}\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(captions):\n",
    "    all_captions = list()\n",
    "    for image_id in captions.keys():\n",
    "        [all_captions.append(caption) for caption in captions[image_id]]\n",
    "    return all_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def create_tokenizer(captions):\n",
    "    lines = to_lines(captions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(captions):\n",
    "    lines = to_lines(captions)\n",
    "    return max(len(line.split()) for line in lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, captions_list, image):\n",
    "    # X1 : input for image features\n",
    "    # X2 : input for text features\n",
    "    # y  : output word\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    # Walk through each caption for the image\n",
    "    for caption in captions_list:\n",
    "        # Encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        # Split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # Split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # Pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # Encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # Store\n",
    "            X1.append(image)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return X1, X2, y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images, captions, tokenizer, max_length, batch_size, random_seed):\n",
    "    # Setting random seed for reproducibility of results\n",
    "    random.seed(random_seed)\n",
    "    # Image ids\n",
    "    image_ids = list(captions.keys())\n",
    "    _count = 0\n",
    "    assert batch_size <= len(\n",
    "        image_ids), 'Batch size must be less than or equal to {}'.format(len(image_ids))\n",
    "    while True:\n",
    "        if _count >= len(image_ids):\n",
    "            # Generator exceeded or reached the end so restart it\n",
    "            _count = 0\n",
    "        # Batch list to store data\n",
    "        input_img_batch, input_sequence_batch, output_word_batch = list(), list(), list()\n",
    "        for i in range(_count, min(len(image_ids), _count+batch_size)):\n",
    "            # Retrieve the image id\n",
    "            image_id = image_ids[i]\n",
    "            # Retrieve the image features\n",
    "            image = images[image_id][0]\n",
    "            # Retrieve the captions list\n",
    "            captions_list = captions[image_id]\n",
    "            # Shuffle captions list\n",
    "            random.shuffle(captions_list)\n",
    "            input_img, input_sequence, output_word = create_sequences(\n",
    "                tokenizer, max_length, captions_list, image)\n",
    "            # Add to batch\n",
    "            for j in range(len(input_img)):\n",
    "                input_img_batch.append(input_img[j])\n",
    "                input_sequence_batch.append(input_sequence[j])\n",
    "                output_word_batch.append(output_word[j])\n",
    "        _count = _count + batch_size\n",
    "        yield [[np.array(input_img_batch), np.array(input_sequence_batch)], np.array(output_word_batch)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainData():\n",
    "    train_image_ids = load_set(config['train_data_path'])\n",
    "    # Check if we already have preprocessed data saved and if not, preprocess the data.\n",
    "    # Create and save 'captions.txt' & features.pkl\n",
    "    preprocessData()\n",
    "    # Load captions\n",
    "    train_captions, _count = load_cleaned_captions(\n",
    "        config['model_data_path']+'captions.txt', train_image_ids)\n",
    "    # Load image features\n",
    "    train_image_features = load_image_features(\n",
    "        config['model_data_path']+'features_'+config['model_type']+'.joblib')\n",
    "    print('Available images for training: '+len(train_image_features))\n",
    "    print('Available captions for training: '+_count)\n",
    "    if not os.path.exists(config['model_data_path']+'tokenizer.pkl'):\n",
    "        # Prepare tokenizer\n",
    "        tokenizer = create_tokenizer(train_captions)\n",
    "        # Save the tokenizer\n",
    "        joblib.dump(tokenizer, config['model_data_path']+'tokenizer.joblib')\n",
    "    # Determine the maximum sequence length\n",
    "    max_length = calc_max_length(train_captions)\n",
    "    return train_image_features, train_captions, max_length\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadValData():\n",
    "    val_image_ids = load_set(config['val_data_path'])\n",
    "    # Load captions\n",
    "    val_captions, _count = load_cleaned_captions(\n",
    "        config['model_data_path']+'captions.txt', val_image_ids)\n",
    "    # Load image features\n",
    "    val_features = load_image_features(\n",
    "        config['model_data_path']+'features_'+config['model_type']+'.joblib', val_image_ids)\n",
    "    print('Available images for validation: '+len(val_features))\n",
    "    print('Available captions for validation: '+_count)\n",
    "    return val_features, val_captions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.171174,
     "end_time": "2020-10-25T05:43:36.294434",
     "exception": false,
     "start_time": "2020-10-25T05:43:36.123260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LSTM Model for Training and Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-25T05:43:37.555698Z",
     "iopub.status.busy": "2020-10-25T05:43:37.554649Z",
     "iopub.status.idle": "2020-10-25T05:43:38.476738Z",
     "shell.execute_reply": "2020-10-25T05:43:38.475663Z"
    },
    "papermill": {
     "duration": 1.106921,
     "end_time": "2020-10-25T05:43:38.476863",
     "exception": false,
     "start_time": "2020-10-25T05:43:37.369942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LSTM_Model(vocab_size, max_len, model_type):\n",
    "    embedding_size = lstmConfig['embedding_size']\n",
    "    dimension = 2048\n",
    "    if model_type == 'vgg16':\n",
    "        # VGG16 outputs a 4096 dimensional vector for each image, which we'll feed to RNN Model\n",
    "        dimension = 4096\n",
    "    image_model = Sequential()\n",
    "\n",
    "    image_model.add(\n",
    "        Dense(embedding_size, input_shape=(dimension,), activation='relu'))\n",
    "    image_model.add(RepeatVector(max_len))\n",
    "\n",
    "    image_model.summary()\n",
    "\n",
    "    language_model = Sequential()\n",
    "\n",
    "    language_model.add(Embedding(input_dim=vocab_size,\n",
    "                                 output_dim=embedding_size, input_length=max_len))\n",
    "    language_model.add(LSTM(256, return_sequences=True))\n",
    "    language_model.add(TimeDistributed(Dense(embedding_size)))\n",
    "\n",
    "    language_model.summary()\n",
    "\n",
    "    conca = Concatenate()([image_model.output, language_model.output])\n",
    "    x = LSTM(128, return_sequences=True)(conca)\n",
    "    x = LSTM(512, return_sequences=False)(x)\n",
    "    x = Dense(vocab_size)(x)\n",
    "    out = Activation('softmax')(x)\n",
    "    model = Model(inputs=[image_model.input,\n",
    "                  language_model.input], outputs=out)\n",
    "\n",
    "    # model.load_weights(\"mine_model_weights.h5\")\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, tokenizer, image, max_length):\n",
    "    # Seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # Iterate over the whole length of the sequence\n",
    "    for _ in range(max_length):\n",
    "        # Integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # Pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # Predict next word\n",
    "        # The model will output a prediction, which will be a probability distribution over all words in the vocabulary.\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # The output vector representins a probability distribution where maximum probability is the predicted word position\n",
    "        # Take output class with maximum probability and convert to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # Map integer back to word\n",
    "        word = int_to_word(yhat, tokenizer)\n",
    "        # Stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # Append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # Stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, images, captions, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    for image_id, caption_list in tqdm(captions.items()):\n",
    "        yhat = generate_caption(model, tokenizer, images[image_id], max_length)\n",
    "        ground_truth = [caption.split() for caption in caption_list]\n",
    "        actual.append(ground_truth)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU Scores :')\n",
    "    print('A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.')\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' %\n",
    "          corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed for reproducibility of results\n",
    "random.seed(config['random_seed'])\n",
    "\n",
    "X1train, X2train, max_length = loadTrainData()\n",
    "\n",
    "X1val, X2val = loadValData()\n",
    "\n",
    "tokenizer = joblib.load(config['tokenizer_path'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# model = RNNModel(vocab_size, max_length, rnnConfig, config['model_type'])\n",
    "model = LSTM_Model(vocab_size, max_length, config['model_type'])\n",
    "print('LSTM_Model Summary : ')\n",
    "print(model.summary())\n",
    "\n",
    "\"\"\"\n",
    "    *Train the model save after each epoch\n",
    "\"\"\"\n",
    "num_of_epochs = config['num_of_epochs']\n",
    "batch_size = config['batch_size']\n",
    "steps_train = len(X2train)//batch_size\n",
    "if len(X2train) % batch_size != 0:\n",
    "    steps_train = steps_train+1\n",
    "steps_val = len(X2val)//batch_size\n",
    "if len(X2val) % batch_size != 0:\n",
    "    steps_val = steps_val+1\n",
    "model_save_path = config['model_data_path']+\"model_\"+str(\n",
    "    config['model_type'])+\"_epoch-{epoch:02d}_train_loss-{loss:.4f}_val_loss-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "print('steps_train: {}, steps_val: {}'.format(steps_train, steps_val))\n",
    "print('Batch Size: {}'.format(batch_size))\n",
    "print('Total Number of Epochs = {}'.format(num_of_epochs))\n",
    "\n",
    "# Shuffle train data\n",
    "ids_train = list(X2train.keys())\n",
    "random.shuffle(ids_train)\n",
    "X2train_shuffled = {_id: X2train[_id] for _id in ids_train}\n",
    "X2train = X2train_shuffled\n",
    "\n",
    "# Create the train data generator\n",
    "# returns [[img_features, text_features], out_word]\n",
    "generator_train = data_generator(\n",
    "    X1train, X2train, tokenizer, max_length, batch_size, config['random_seed'])\n",
    "# Create the validation data generator\n",
    "# returns [[img_features, text_features], out_word]\n",
    "generator_val = data_generator(\n",
    "    X1val, X2val, tokenizer, max_length, batch_size, config['random_seed'])\n",
    "\n",
    "# Fit for one epoch\n",
    "history = model.fit_generator(generator_train,\n",
    "                              epochs=num_of_epochs,\n",
    "                              steps_per_epoch=steps_train,\n",
    "                              validation_data=generator_val,\n",
    "                              validation_steps=steps_val,\n",
    "                              callbacks=callbacks,\n",
    "                              verbose=1)\n",
    "\n",
    "\"\"\"\n",
    "\t*Evaluate the model on validation data and ouput BLEU score\n",
    "\"\"\"\n",
    "print('Model trained successfully. Running model on validation set for calculating BLEU score ')\n",
    "evaluate_model(model, X1val, X2val, tokenizer, max_length)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "papermill": {
   "duration": 920.593543,
   "end_time": "2020-10-25T05:56:47.084717",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-25T05:41:26.491174",
   "version": "2.1.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f53d944b2b75f961eba6ec6b49ae11eee88bebce38fd8cba5f9c225ee95284a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
